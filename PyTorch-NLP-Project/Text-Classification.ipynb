{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace # 管理所有超参数\n",
    "from collections import Counter # 统计词语数量\n",
    "import string # 标点符号调用\n",
    "import re # 正则表达式\n",
    "import os # 生成文件路径\n",
    "import json # 保存模型为JSON格式\n",
    "import numpy as np # 数据处理\n",
    "import pandas as pd # 文本处理\n",
    "import torch # 调用PyTorch库\n",
    "import torch.nn as nn # 调用神经网络层\n",
    "import torch.nn.functional as F # 调用激活函数\n",
    "import torch.optim as optim # 调用优化器\n",
    "from torch.utils.data import Dataset, DataLoader # 调用批生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    news_csv=\"20200913-Top10-clean\", # 数据集\n",
    "    vectorizer_file=\"news_folder/vectorizer.json\", # 向量化器保存的位置\n",
    "    model_state_file=\"news_folder/model.pth\", # 模型保存的位置\n",
    "    predicted_file=\"news_folder/predicted.csv\", # 预测文件的位置\n",
    "    word_embedding_size=100, # 词向量维度大小\n",
    "    rnn_hidden_size=64, # RNN的隐藏层大小\n",
    "    num_epochs=20, # 模型训练轮数\n",
    "    learning_rate=1e-3, # 学习率\n",
    "    batch_size=64, # 批的大小\n",
    "    seed=1337, # 设置种子\n",
    "    early_stopping_criteria=3, # 超过未优化次数将停止训练\n",
    "    sampling=0.05, # 用CPU训练时取样5%数据用于训练\n",
    "    cutoff=2, # 设置词典中词的最小频率\n",
    "    dropout=0.5 # dropout的概率\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    '''创建一个词典类来管理数据集中每个词和对应索引的关系'''\n",
    "    \n",
    "    def __init__(self, token_to_idx={}):\n",
    "        '''\n",
    "        Args:\n",
    "            token_to_idx: 载入预先生成好的词典，若没有会自动生成空词典\n",
    "        '''\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "    # 向双向词典中加入令牌，并返回令牌在词典中所在的索引，若令牌已存在，直接返回索引\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    # 查找令牌在词典中的对应索引\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    # 查找索引在词典中对应的令牌，若索引不存在将报错\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    # 通过使用contents(序列化后的初始化信息)重建实例\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    # Print打印实例的输出结果\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    # 定义实例的长度信息为词典的长度\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    \n",
    "    '''创建一个词典类来管理数据集中每个词和对应索引的关系'''\n",
    "\n",
    "    def __init__(self, token_to_idx={}, unk_token='<UNK>', mask_token='<MASK>',\n",
    "                 begin_token='<BEGIN>', end_token='<END>'):\n",
    "        '''\n",
    "        Args:\n",
    "            token_to_idx: 载入预先生成好的词典，若没有会自动生成空词典\n",
    "            unk_token，mask_token，begin_token, end_token: 文本中的特殊令牌\n",
    "        '''\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        # 保存特殊的令牌\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_token = begin_token\n",
    "        self._end_token = end_token\n",
    "        \n",
    "        # 将特殊令牌添加到词典中，并保存对应的索引\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_index = self.add_token(self._begin_token)\n",
    "        self.end_index = self.add_token(self._end_token)\n",
    "\n",
    "    # 查找令牌在词典中对应的索引，如果令牌不存在，则返回UNK索引\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_token': self._begin_token,\n",
    "                         'end_token': self._end_token})\n",
    "        return contents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \n",
    "    '''创建一个向量化器类将文本句子转换为句子索引向量'''\n",
    "    \n",
    "    def __init__(self, word_vocab, label_vocab):\n",
    "        '''Args:\n",
    "               word_vocab: 包含数据集中所有文本的词典\n",
    "               label_vocab: 包含数据集中所有标签的词典\n",
    "        '''\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        \n",
    "    # 向量化文本句子，将句子中的每个单词用索引表示，生成句子索引向量\n",
    "    def vectorize(self, news_title, vector_length=-1):\n",
    "        indices = [self.word_vocab.begin_index]\n",
    "        indices.extend([self.word_vocab.lookup_token(token) \n",
    "                       for token in news_title.split(' ')])\n",
    "        indices.append(self.word_vocab.end_index)\n",
    "        if vector_length == -1:\n",
    "            vector_length = len(indices)\n",
    "        title_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        title_vector[:len(indices)] = indices\n",
    "        title_vector[len(indices):] = self.word_vocab.mask_index\n",
    "        return title_vector, len(indices)\n",
    "    \n",
    "    # 通过新闻数据集创建一个向量化器\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=args.cutoff):\n",
    "        label_vocab = Vocabulary()\n",
    "        for label in sorted(set(news_df['一级类目'])):\n",
    "            label_vocab.add_token(label)\n",
    "        word_counts = Counter()\n",
    "        for title in news_df['新闻标题']:\n",
    "            for token in title.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1 \n",
    "        word_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                word_vocab.add_token(word)\n",
    "        return cls(word_vocab, label_vocab)\n",
    "\n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        return {'word_vocab': self.word_vocab.to_serializable(), \n",
    "                'label_vocab': self.label_vocab.to_serializable()}\n",
    "    \n",
    "    # 通过使用contents(序列化后的初始化信息)重建实例\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        word_vocab = SequenceVocabulary.from_serializable(contents['word_vocab'])\n",
    "        label_vocab = Vocabulary.from_serializable(contents['label_vocab'])\n",
    "        return cls(word_vocab, label_vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \n",
    "    '''创建一个新闻数据类来对数据进行向量化和分组'''\n",
    "    \n",
    "    def __init__(self, news_df, vectorizer):\n",
    "        '''\n",
    "        Args:\n",
    "            news_df: 新闻数据集\n",
    "            vectorizer: 由训练集生成的向量化器\n",
    "        '''\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "        # 计算数据集中最长文本的长度，用于之后的句子向量化\n",
    "        self._max_seq_length = max(map(len, self.news_df[\"新闻标题\"])) + 2\n",
    "        self.train_df = self.news_df.iloc[0:int(len(self.news_df)*0.7)]\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.news_df.iloc[int(len(self.news_df)*0.7):int(len(self.news_df)*0.85)]\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.news_df.iloc[int(len(self.news_df)*0.85):]\n",
    "        self.test_size = len(self.test_df)\n",
    "        # 将数据集分划后保存在dict中，通过set_split调取需要使用的数据集\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 由于一级类目的样本不平衡而计算的样本权重，用于softmax加权\n",
    "        class_counts = self.train_df['一级类目'].value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.label_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    # 根据情况选择当前要使用的数据集，默认使用训练集\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "         \n",
    "    # 定义数据集的长度，用于DataLoader的batch数量计算\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "        \n",
    "    # 定义数据集的输出，用于DataLoader的batch数据生成\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        news_vector, vec_length = \\\n",
    "        self._vectorizer.vectorize(row[\"新闻标题\"], self._max_seq_length)\n",
    "        label_index = \\\n",
    "        self._vectorizer.label_vocab.lookup_token(row[\"一级类目\"])\n",
    "        return {'x_data': news_vector, \n",
    "                'y_target': label_index, \n",
    "                'x_length': vec_length}\n",
    "        \n",
    "    # 用于之后的vectorizer提取使用\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "        \n",
    "    # 通过新闻数据集创建数据集实例\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv, sampling):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        news_df = news_df.iloc[0:int(len(news_df)*sampling)]\n",
    "        train_df = news_df.iloc[0:int(len(news_df)*0.7)]\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_df))\n",
    "        \n",
    "    # 通过数据集以及保存好的向量化器来创建数据集实例\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath, sampling):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        news_df = news_df.iloc[0:int(len(news_df)*sampling)]\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_df, vectorizer)\n",
    "        \n",
    "    # 从JSON文件中加载保存好的向量化器\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    # 将向量化器保存到JSON文件中\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    \n",
    "    '''使用RNNCell创建一个RNN层'''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        '''\n",
    "        Args:\n",
    "            input_size: RNN输入数据的维度\n",
    "            hidden_size: RNN的隐藏层大小\n",
    "            batch_first: batch是否为数据集的第0维\n",
    "        '''\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    # 初始化隐藏层数值\n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "    # 输入数据批，返回每一个时间步长上的隐藏层数值\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "        hiddens = []\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "        hidden_t = initial_hidden\n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "        return hiddens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    \n",
    "    '''创建新闻标题分类器'''\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx,\n",
    "                 rnn_hidden_size, num_classes, batch_first):\n",
    "        '''\n",
    "        Args:\n",
    "            num_embeddings: 词嵌入矩阵的行数，等于词典单词的数量\n",
    "            embedding_dim: 词嵌入矩阵的维度，人为规定大小\n",
    "            padding_idx: 将某个index对应的令牌作为padding对象\n",
    "            rnn_hidden_size: RNN的隐藏层大小\n",
    "            num_classes: 输出层的大小\n",
    "            batch_first: batch是否为数据集的第0维\n",
    "        '''\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                embedding_dim=embedding_dim,\n",
    "                                padding_idx=padding_idx)\n",
    "        self.rnn = ElmanRNN(input_size=embedding_dim,\n",
    "                            hidden_size=rnn_hidden_size,\n",
    "                            batch_first=batch_first)\n",
    "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                             out_features=rnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
    "                             out_features=num_classes)\n",
    "    \n",
    "    # 输入数据批，返回批的最后一个时间步长上的隐藏层数值\n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        x_embedded = self.emb(x_in)\n",
    "        y_out = self.rnn(x_embedded)\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "            out = []\n",
    "            for batch_index, column_index in enumerate(x_lengths):\n",
    "                out.append(y_out[batch_index, column_index])\n",
    "            y_out = torch.stack(out)\n",
    "        else:\n",
    "            y_out = y_out[:, -1, :]\n",
    "        y_out = F.relu(self.fc1(F.dropout(y_out, args.dropout)))\n",
    "        y_out = self.fc2(F.dropout(y_out, args.dropout))\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "        return y_out  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来跟踪模型的训练过程以及控制训练状态\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每轮训练结束将更新一次训练状态\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_pre_t, loss_t = train_state['val_loss'][-2:]\n",
    "        if loss_t >= loss_pre_t:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        else:\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "            train_state['early_stopping_step'] = 0\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的准确度\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型，验证模型，测试模型，保存模型\n",
    "def train_model(args, dataset, classifier, optimizer, loss_func, scheduler):\n",
    "    train_state = make_train_state(args)\n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "            # 训连模型并将每轮训练结果用于更新状态\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = DataLoader(dataset=dataset, batch_size=args.batch_size)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            classifier.train()\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "            train_state['train_acc'].append(running_acc)\n",
    "            # 验证模型并将验证结果用于更新状态\n",
    "            dataset.set_split('val')\n",
    "            batch_generator = DataLoader(dataset=dataset, batch_size=args.batch_size)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            classifier.eval()\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "            # 更新训练状态\n",
    "            train_state = update_train_state(args=args, model=classifier, train_state=train_state)\n",
    "            # 更新学习率\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "            # 打印每轮训练的结果\n",
    "            print(\"Epoch: {} / {} -- Train Accuracy: {:.3f}, Val Accuracy: {:.3f}\".format(\n",
    "                train_state['epoch_index']+1, args.num_epochs, train_state['train_acc'][-1], \n",
    "                train_state['val_acc'][-1]))\n",
    "            # 判断是否提前结速训练\n",
    "            if train_state['stop_early']:\n",
    "                print('Early Stop Training!')\n",
    "                break\n",
    "        # 使用测试集测试训练好的模型，更新状态中的测试结果\n",
    "        classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "        loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "        dataset.set_split('test')\n",
    "        batch_generator = DataLoader(dataset=dataset, batch_size=args.batch_size)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.eval()\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred = classifier(batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        train_state['test_loss'] = running_loss\n",
    "        train_state['test_acc'] = running_acc\n",
    "        print(\"Test Accuracy: {:.3f}\".format(train_state['test_acc']))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练模型需要的所有工具\n",
    "def prepare_training_process(args):\n",
    "    # 设置概率种子\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # 初始化保存文件夹\n",
    "    if not os.path.exists('news_folder/'):\n",
    "        os.makedirs('news_folder/')\n",
    "    # 准备数据集\n",
    "    if os.path.exists(args.vectorizer_file):\n",
    "        dataset = NewsDataset.load_dataset_and_load_vectorizer(\n",
    "            args.news_csv, args.vectorizer_file, args.sampling)\n",
    "    else:\n",
    "        dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv, args.sampling)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "    # 准备向量化器\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    # 准备分类器\n",
    "    classifier = NewsClassifier(\n",
    "                 num_embeddings=len(vectorizer.word_vocab),\n",
    "                 embedding_dim=args.word_embedding_size,\n",
    "                 padding_idx=vectorizer.word_vocab.mask_index,\n",
    "                 rnn_hidden_size=args.rnn_hidden_size,\n",
    "                 num_classes=len(vectorizer.label_vocab),\n",
    "                 batch_first=True)\n",
    "    # 准备优化器\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "    # 准备损失函数\n",
    "    loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "    # 准备学习率调整器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "    return dataset, vectorizer, classifier, optimizer, loss_func, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练分类器模型，并保存到文件夹中\n",
    "dataset, vectorizer, classifier, optimizer, loss_func, scheduler = prepare_training_process(args)\n",
    "train_model(args, dataset, classifier, optimizer, loss_func, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(classifier, vectorizer):\n",
    "    news_df = pd.read_csv(args.news_csv)\n",
    "    news_df = news_df.iloc[0:int(len(news_df)*0.01)]\n",
    "    predicted_news_df = []\n",
    "    for news_title in news_df['新闻标题']:\n",
    "        vectorized_news, vec_length = vectorizer.vectorize(news_title)\n",
    "        vectorized_news = torch.tensor(vectorized_news).unsqueeze(dim=0)\n",
    "        vec_length = torch.tensor([vec_length], dtype=torch.int64)\n",
    "        result = classifier(vectorized_news, vec_length, apply_softmax=True)\n",
    "        probability_values, indices = result.max(dim=1)\n",
    "        index = indices.item()\n",
    "        prob_value = probability_values.item()\n",
    "        predicted_label = vectorizer.label_vocab.lookup_index(index)\n",
    "        predicted_result = {'news_title': news_title, 'label': predicted_label, 'probability': prob_value}\n",
    "        predicted_news_df.append(predicted_result)\n",
    "    output_df = pd.DataFrame(predicted_news_df)\n",
    "    output_df.to_csv(args.predicted_file, index=False)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = predict_label(classifier, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
