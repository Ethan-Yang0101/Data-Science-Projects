{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace # 管理所有超参数\n",
    "from collections import Counter # 统计字的数量\n",
    "import string # 标点符号调用\n",
    "import re # 正则表达式\n",
    "import os # 生成文件路径\n",
    "import json # 保存模型为JSON格式\n",
    "import numpy as np # 数据处理\n",
    "import pandas as pd # 文本处理\n",
    "import torch # 调用PyTorch库\n",
    "import torch.nn as nn # 调用神经网络层\n",
    "import torch.nn.functional as F # 调用激活函数\n",
    "import torch.optim as optim # 调用优化器\n",
    "from torch.utils.data import Dataset, DataLoader # 调用批生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    news_csv=\"20200913-Top10-clean-char\", # 数据集\n",
    "    vectorizer_file=\"news_folder/vectorizer1.json\", # 向量化器保存的位置\n",
    "    model_state_file=\"news_folder/model1.pth\", # 模型保存的位置\n",
    "    predicted_file=\"news_folder/predicted1.csv\", # 预测文件的位置\n",
    "    char_embedding_size=100, # 字向量维度大小\n",
    "    rnn_hidden_size=64, # RNN的隐藏层大小\n",
    "    num_epochs=20, # 模型训练轮数\n",
    "    learning_rate=1e-3, # 学习率\n",
    "    batch_size=64, # 批的大小\n",
    "    seed=1337, # 设置种子\n",
    "    early_stopping_criteria=3, # 超过未优化次数将停止训练\n",
    "    sampling=0.1, # 用CPU训练时取样50%数据用于训练\n",
    "    cutoff=1, # 设置字典中字的最小频率\n",
    "    dropout=0.5 # dropout的概率\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    '''创建一个字典类来管理数据集中每个字和对应索引的关系'''\n",
    "    \n",
    "    def __init__(self, token_to_idx={}):\n",
    "        '''\n",
    "        Args:\n",
    "            token_to_idx: 载入预先生成好的字典，若没有会自动生成空字典\n",
    "        '''\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "    # 向双向字典中加入令牌，并返回令牌在字典中所在的索引，若令牌已存在，直接返回索引\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    # 查找令牌在字典中的对应索引\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    # 查找索引在字典中对应的令牌，若索引不存在将报错\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    # 通过使用contents(序列化后的初始化信息)重建实例\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    # Print打印实例的输出结果\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    # 定义实例的长度信息为字典的长度\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    \n",
    "    '''创建一个字典类来管理数据集中每个字和对应索引的关系'''\n",
    "\n",
    "    def __init__(self, token_to_idx={}, unk_token='<UNK>', mask_token='<MASK>',\n",
    "                 begin_token='<BEGIN>', end_token='<END>'):\n",
    "        '''\n",
    "        Args:\n",
    "            token_to_idx: 载入预先生成好的字典，若没有会自动生成空字典\n",
    "            unk_token，mask_token，begin_token, end_token: 文本中的特殊令牌\n",
    "        '''\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        # 保存特殊的令牌\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_token = begin_token\n",
    "        self._end_token = end_token\n",
    "        \n",
    "        # 将特殊令牌添加到字典中，并保存对应的索引\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_index = self.add_token(self._begin_token)\n",
    "        self.end_index = self.add_token(self._end_token)\n",
    "\n",
    "    # 查找令牌在字典中对应的索引，如果令牌不存在，则返回UNK索引\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_token': self._begin_token,\n",
    "                         'end_token': self._end_token})\n",
    "        return contents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \n",
    "    '''创建一个向量化器类将文本句子转换为句子索引向量'''\n",
    "    \n",
    "    def __init__(self, char_vocab, label_vocab):\n",
    "        '''Args:\n",
    "               char_vocab: 包含数据集中所有文本的字典\n",
    "               label_vocab: 包含数据集中所有标签的字典\n",
    "        '''\n",
    "        self.char_vocab = char_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        \n",
    "    # 向量化文本句子，将句子中的每个字用索引表示，生成用于训练和预测的句子索引向量\n",
    "    def vectorize(self, news_title, vector_length=-1):\n",
    "        indices = [self.char_vocab.begin_index]\n",
    "        indices.extend([self.char_vocab.lookup_token(token) \n",
    "                       for token in news_title])\n",
    "        indices.append(self.char_vocab.end_index)\n",
    "        if vector_length == -1:\n",
    "            vector_length = len(indices)\n",
    "        from_vector = np.empty(vector_length, dtype=np.int64)         \n",
    "        from_indices = indices[:-1]\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        from_vector[len(from_indices):] = self.char_vocab.mask_index\n",
    "        to_vector = np.empty(vector_length, dtype=np.int64)\n",
    "        to_indices = indices[1:]\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        to_vector[len(to_indices):] = self.char_vocab.mask_index\n",
    "        return from_vector, to_vector\n",
    "    \n",
    "    # 通过新闻数据集创建一个向量化器\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=25):\n",
    "        label_vocab = Vocabulary()\n",
    "        for label in sorted(set(news_df['一级类目'])):\n",
    "            label_vocab.add_token(label)\n",
    "        char_counts = Counter()\n",
    "        for title in news_df['新闻标题']:\n",
    "            for token in title:\n",
    "                if token not in string.punctuation:\n",
    "                    char_counts[token] += 1 \n",
    "        char_vocab = SequenceVocabulary()\n",
    "        for char, char_count in char_counts.items():\n",
    "            if char_count >= cutoff:\n",
    "                char_vocab.add_token(char)\n",
    "        return cls(char_vocab, label_vocab)\n",
    "\n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        return {'char_vocab': self.char_vocab.to_serializable(), \n",
    "                'label_vocab': self.label_vocab.to_serializable()}\n",
    "    \n",
    "    # 通过使用contents(序列化后的初始化信息)重建实例\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
    "        label_vocab = Vocabulary.from_serializable(contents['label_vocab'])\n",
    "        return cls(char_vocab, label_vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \n",
    "    '''创建一个新闻数据类来对数据进行向量化和分组'''\n",
    "    \n",
    "    def __init__(self, news_df, vectorizer):\n",
    "        '''\n",
    "        Args:\n",
    "            news_df: 新闻数据集\n",
    "            vectorizer: 由训练集生成的向量化器\n",
    "        '''\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "        # 计算数据集中最长文本的长度，用于之后的句子向量化\n",
    "        self._max_seq_length = max(map(len, self.news_df[\"新闻标题\"])) + 2\n",
    "        self.train_df = self.news_df.iloc[0:int(len(self.news_df)*0.7)]\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.news_df.iloc[int(len(self.news_df)*0.7):int(len(self.news_df)*0.85)]\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.news_df.iloc[int(len(self.news_df)*0.85):]\n",
    "        self.test_size = len(self.test_df)\n",
    "        # 将数据集分划后保存在dict中，通过set_split调取需要使用的数据集\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # 由于一级类目的样本不平衡而计算的样本权重，用于softmax加权\n",
    "        class_counts = self.train_df['一级类目'].value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.label_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    # 根据情况选择当前要使用的数据集，默认使用训练集\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "         \n",
    "    # 定义数据集的长度，用于DataLoader的batch数量计算\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "        \n",
    "    # 定义数据集的输出，用于DataLoader的batch数据生成\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        from_vector, to_vector = \\\n",
    "        self._vectorizer.vectorize(row[\"新闻标题\"], self._max_seq_length)\n",
    "        label_index = \\\n",
    "        self._vectorizer.label_vocab.lookup_token(row[\"一级类目\"])\n",
    "        return {'x_data': from_vector, \n",
    "                'y_target': to_vector, \n",
    "                'label_index': label_index}\n",
    "        \n",
    "    # 用于之后的vectorizer提取使用\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "        \n",
    "    # 通过新闻数据集创建数据集实例\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv, sampling):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        news_df = news_df.iloc[0:int(len(news_df)*sampling)]\n",
    "        train_df = news_df.iloc[0:int(len(news_df)*0.7)]\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_df))\n",
    "        \n",
    "    # 通过数据集以及保存好的向量化器来创建数据集实例\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath, sampling):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        news_df = news_df.iloc[0:int(len(news_df)*sampling)]\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_df, vectorizer)\n",
    "        \n",
    "    # 从JSON文件中加载保存好的向量化器\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    # 将向量化器保存到JSON文件中\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGenerationModel(nn.Module):\n",
    "    \n",
    "    '''创建新闻序列生成模型'''\n",
    "    \n",
    "    def __init__(self, char_vocab_size, char_embedding_size, rnn_hidden_size,\n",
    "                 num_labels, padding_idx=0, batch_first=True, dropout_p=0.5): \n",
    "        '''\n",
    "        Args:\n",
    "            char_vocab_size: 字嵌入矩阵的行数，等于字典中字的数量\n",
    "            char_embedding_size: 字嵌入矩阵的维度，人为规定大小\n",
    "            rnn_hidden_size: RNN的隐藏层大小\n",
    "            num_labels: 标签的个数\n",
    "            padding_idx: 将某个index对应的令牌作为padding对象，默认为MASk\n",
    "            batch_first: batch是否为数据集的第0维 \n",
    "            dropout_p: 正则化概率\n",
    "        '''\n",
    "        super(NewsGenerationModel, self).__init__()\n",
    "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                     embedding_dim=char_embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "        self.label_emb = nn.Embedding(num_embeddings=num_labels,\n",
    "                                      embedding_dim=rnn_hidden_size)\n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size,\n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
    "                            out_features=char_vocab_size)\n",
    "        self._dropout_p = dropout_p\n",
    "    \n",
    "    # 输入数据批，返回每一个时间步长上的隐藏层数值\n",
    "    def forward(self, x_in, label_index, apply_softmax=False):\n",
    "        x_embedded = self.char_emb(x_in)\n",
    "        label_embedded = self.label_emb(label_index)\n",
    "        y_out, _ = self.rnn(x_embedded, label_embedded)\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))                \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "        return y_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来跟踪模型的训练过程以及控制训练状态\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每轮训练结束将更新一次训练状态\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_pre_t, loss_t = train_state['val_loss'][-2:]\n",
    "        if loss_t >= loss_pre_t:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        else:\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "            train_state['early_stopping_step'] = 0\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的准确度\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "    return n_correct / n_valid * 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的损失值\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练模型需要的所有工具\n",
    "def prepare_training_process(args):\n",
    "    # 设置概率种子\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # 初始化保存文件夹\n",
    "    if not os.path.exists('news_folder/'):\n",
    "        os.makedirs('news_folder/')\n",
    "    # 准备数据集\n",
    "    if os.path.exists(args.vectorizer_file):\n",
    "        dataset = NewsDataset.load_dataset_and_load_vectorizer(\n",
    "            args.news_csv, args.vectorizer_file, args.sampling)\n",
    "    else:\n",
    "        dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv, args.sampling)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "    # 准备向量化器\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    # 准备分类器\n",
    "    model = NewsGenerationModel(\n",
    "                 char_embedding_size=args.char_embedding_size,\n",
    "                 char_vocab_size=len(vectorizer.char_vocab),\n",
    "                 rnn_hidden_size=args.rnn_hidden_size,\n",
    "                 padding_idx=vectorizer.char_vocab.mask_index,\n",
    "                 num_labels=len(vectorizer.label_vocab))\n",
    "    # 准备优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    # 准备损失函数\n",
    "    loss_func = sequence_loss\n",
    "    # 准备学习率调整器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "    return dataset, vectorizer, model, optimizer, loss_func, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型，验证模型，测试模型，保存模型\n",
    "def train_model(args, dataset, model, optimizer, loss_func, scheduler):\n",
    "    mask_index = dataset.get_vectorizer().char_vocab.mask_index\n",
    "    train_state = make_train_state(args)\n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "            # 训连模型并将每轮训练结果用于更新状态\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = DataLoader(dataset=dataset, batch_size=args.batch_size)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            model.train()\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                y_pred = model(x_in=batch_dict['x_data'], label_index=batch_dict['label_index'])\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'], mask_index)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "            train_state['train_acc'].append(running_acc)\n",
    "            # 验证模型并将验证结果用于更新状态\n",
    "            dataset.set_split('val')\n",
    "            batch_generator = DataLoader(dataset=dataset, batch_size=args.batch_size)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            model.eval()\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                y_pred = model(x_in=batch_dict['x_data'], label_index=batch_dict['label_index'])\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'], mask_index)\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "            # 更新训练状态\n",
    "            train_state = update_train_state(args=args, model=model, train_state=train_state)\n",
    "            # 更新学习率\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "            # 打印每轮训练的结果\n",
    "            print(\"Epoch: {} / {} -- Train Accuracy: {:.3f}, Val Accuracy: {:.3f}\".format(\n",
    "                train_state['epoch_index']+1, args.num_epochs, train_state['train_acc'][-1], \n",
    "                train_state['val_acc'][-1]))\n",
    "            # 判断是否提前结速训练\n",
    "            if train_state['stop_early']:\n",
    "                print('Early Stop Training!')\n",
    "                break\n",
    "        # 使用测试集测试训练好的模型，更新状态中的测试结果\n",
    "        model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "        dataset.set_split('test')\n",
    "        batch_generator = DataLoader(dataset=dataset, batch_size=args.batch_size)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.eval()\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred = model(batch_dict['x_data'], label_index=batch_dict['label_index'])\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'], mask_index)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        train_state['test_loss'] = running_loss\n",
    "        train_state['test_acc'] = running_acc\n",
    "        print(\"Test Accuracy: {:.3f}\".format(train_state['test_acc']))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练分类器模型，并保存到文件夹中\n",
    "dataset, vectorizer, model, optimizer, loss_func, scheduler = prepare_training_process(args)\n",
    "train_model(args, dataset, model, optimizer, loss_func, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机生成一定数量的新闻标题的索引句子\n",
    "def sample_from_model(model, vectorizer, label_indices, num_samples=1, sample_size=20):\n",
    "    begin_index = [vectorizer.char_vocab.begin_index for _ in range(num_samples)]\n",
    "    begin_index = torch.tensor(begin_index, dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_index]\n",
    "    label_indices = torch.tensor(label_indices, dtype=torch.int64).unsqueeze(dim=0)\n",
    "    h_t = model.label_emb(label_indices)\n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.char_emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将索引句子解码为新闻标题文字\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    decoded_surnames = []\n",
    "    vocab = vectorizer.char_vocab\n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        surname = \"\"\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            if sample_item == vocab.begin_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_index:\n",
    "                break\n",
    "            else:\n",
    "                surname += vocab.lookup_index(sample_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_titles = 10\n",
    "sampled_surnames = decode_samples(\n",
    "    sample_from_model(model, vectorizer, num_samples=num_titles), \n",
    "    vectorizer)\n",
    "print (\"-\"*15)\n",
    "for i in range(num_titles):\n",
    "    print(sampled_surnames[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
