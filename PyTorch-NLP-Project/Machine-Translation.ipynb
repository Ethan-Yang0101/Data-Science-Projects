{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace # 管理所有超参数\n",
    "from collections import Counter # 统计词语数量\n",
    "import string # 标点符号调用\n",
    "import re # 正则表达式\n",
    "import os # 生成文件路径\n",
    "import json # 保存模型为JSON格式\n",
    "import numpy as np # 数据处理\n",
    "import pandas as pd # 文本处理\n",
    "import torch # 调用PyTorch库\n",
    "import torch.nn as nn # 调用神经网络层\n",
    "import torch.nn.functional as F # 调用激活函数\n",
    "import torch.optim as optim # 调用优化器\n",
    "from torch.utils.data import Dataset, DataLoader # 调用批生成器\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence # 处理变长数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    nmt_csv=\"cmn_data\", # 数据集\n",
    "    vectorizer_file=\"news_folder/vectorizer2.json\", # 向量化器保存的位置\n",
    "    model_state_file=\"news_folder/model2.pth\", # 模型保存的位置\n",
    "    predicted_file=\"news_folder/predicted2.csv\", # 预测文件的位置\n",
    "    char_embedding_size=100, # 字向量维度大小\n",
    "    rnn_hidden_size=64, # RNN的隐藏层大小\n",
    "    num_epochs=20, # 模型训练轮数\n",
    "    learning_rate=5e-4, # 学习率\n",
    "    batch_size=32, # 批的大小\n",
    "    seed=1337, # 设置种子\n",
    "    early_stopping_criteria=3, # 超过未优化次数将停止训练\n",
    "    sampling=0.5, # 用CPU训练时取样50%数据用于训练\n",
    "    source_embedding_size=24, # 需要翻译词嵌入大小\n",
    "    target_embedding_size=24, # 翻译结果词嵌入大小\n",
    "    encoding_size=32 # 编码器隐藏向量大小\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    '''创建一个词典类来管理数据集中每个词和对应索引的关系'''\n",
    "    \n",
    "    def __init__(self, token_to_idx={}):\n",
    "        '''\n",
    "        Args:\n",
    "            token_to_idx: 载入预先生成好的词典，若没有会自动生成空词典\n",
    "        '''\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "    \n",
    "    # 向双向词典中加入令牌，并返回令牌在词典中所在的索引，若令牌已存在，直接返回索引\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    # 查找令牌在词典中的对应索引\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    # 查找索引在词典中对应的令牌，若索引不存在将报错\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    # 通过使用contents(序列化后的初始化信息)重建实例\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    # Print打印实例的输出结果\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    # 定义实例的长度信息为词典的长度\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    \n",
    "    '''创建一个词典类来管理数据集中每个词和对应索引的关系'''\n",
    "\n",
    "    def __init__(self, token_to_idx={}, unk_token='<UNK>', mask_token='<MASK>',\n",
    "                 begin_token='<BEGIN>', end_token='<END>'):\n",
    "        '''\n",
    "        Args:\n",
    "            token_to_idx: 载入预先生成好的词典，若没有会自动生成空词典\n",
    "            unk_token，mask_token，begin_token, end_token: 文本中的特殊令牌\n",
    "        '''\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        \n",
    "        # 保存特殊的令牌\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_token = begin_token\n",
    "        self._end_token = end_token\n",
    "        \n",
    "        # 将特殊令牌添加到词典中，并保存对应的索引\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_index = self.add_token(self._begin_token)\n",
    "        self.end_index = self.add_token(self._end_token)\n",
    "\n",
    "    # 查找令牌在词典中对应的索引，如果令牌不存在，则返回UNK索引\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_token': self._begin_token,\n",
    "                         'end_token': self._end_token})\n",
    "        return contents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTVectorizer(object):\n",
    "    \n",
    "    '''创建一个向量化器类将文本句子转换为句子索引向量'''\n",
    "    \n",
    "    def __init__(self, source_vocab, target_vocab, max_source_length, max_target_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_vocab: 包含数据集中所有需要翻译的文本词典\n",
    "            target_vocab: 包含数据集中所有翻译结果的文本词典\n",
    "            max_source_length: 需要翻译的文本词典中词的最大长度\n",
    "            max_target_length: 翻译结果的文本词典中词的最大长度\n",
    "        \"\"\"\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    # 基本的句子向量化过程\n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        vector[len(indices):] = mask_index\n",
    "        return vector\n",
    "    \n",
    "    # 从文本中获取需要翻译的句子索引\n",
    "    def _get_source_indices(self, text):\n",
    "        indices = [self.source_vocab.begin_index]\n",
    "        indices.extend(self.source_vocab.lookup_token(token) for token in text.split(\" \"))\n",
    "        indices.append(self.source_vocab.end_index)\n",
    "        return indices\n",
    "    \n",
    "    # 从文本中获取翻译结果的句子索引（序列模型的输入和输出）\n",
    "    def _get_target_indices(self, text):\n",
    "        indices = [self.target_vocab.lookup_token(token) for token in text.split(\" \")]\n",
    "        x_indices = [self.target_vocab.begin_index] + indices\n",
    "        y_indices = indices + [self.target_vocab.end_index]\n",
    "        return x_indices, y_indices\n",
    "        \n",
    "    # 向量化文本句子，将句子中的每个单词用索引表示，生成句子索引向量\n",
    "    def vectorize(self, source_text, target_text, use_dataset_max_lengths=True):\n",
    "        source_vector_length = -1\n",
    "        target_vector_length = -1\n",
    "        if use_dataset_max_lengths:\n",
    "            source_vector_length = self.max_source_length + 2\n",
    "            target_vector_length = self.max_target_length + 1\n",
    "        source_indices = self._get_source_indices(source_text)\n",
    "        source_vector = self._vectorize(source_indices, \n",
    "                                        vector_length=source_vector_length, \n",
    "                                        mask_index=self.source_vocab.mask_index)\n",
    "        target_x_indices, target_y_indices = self._get_target_indices(target_text)\n",
    "        target_x_vector = self._vectorize(target_x_indices,\n",
    "                                          vector_length=target_vector_length,\n",
    "                                          mask_index=self.target_vocab.mask_index)\n",
    "        target_y_vector = self._vectorize(target_y_indices,\n",
    "                                          vector_length=target_vector_length,\n",
    "                                          mask_index=self.target_vocab.mask_index)\n",
    "        return {\"source_vector\": source_vector, \n",
    "                \"target_x_vector\": target_x_vector, \n",
    "                \"target_y_vector\": target_y_vector, \n",
    "                \"source_length\": len(source_indices)}\n",
    "     \n",
    "    # 通过新闻数据集创建一个向量化器\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, bitext_df):\n",
    "        source_vocab = SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "        max_source_length = max(map(len, bitext_df['English']))\n",
    "        max_target_length = max(map(len, bitext_df['Chinese']))\n",
    "        bitext_df = bitext_df.iloc[0:int(len(bitext_df)*0.7)]\n",
    "        for _, row in bitext_df.iterrows():\n",
    "            source_tokens = row[\"English\"].split(\" \")\n",
    "            for token in source_tokens:\n",
    "                source_vocab.add_token(token)\n",
    "            target_tokens = row[\"Chinese\"].split(\" \")\n",
    "            for token in target_tokens:\n",
    "                target_vocab.add_token(token)\n",
    "        return cls(source_vocab, target_vocab, max_source_length, max_target_length)\n",
    "\n",
    "    # 通过使用contents(序列化后的初始化信息)重建实例\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        source_vocab = SequenceVocabulary.from_serializable(contents[\"source_vocab\"])\n",
    "        target_vocab = SequenceVocabulary.from_serializable(contents[\"target_vocab\"])\n",
    "        return cls(source_vocab=source_vocab, \n",
    "                   target_vocab=target_vocab, \n",
    "                   max_source_length=contents[\"max_source_length\"], \n",
    "                   max_target_length=contents[\"max_target_length\"])\n",
    "\n",
    "    # 生成序列化信息，方便使用JSON保存初始化信息\n",
    "    def to_serializable(self):\n",
    "        return {\"source_vocab\": self.source_vocab.to_serializable(), \n",
    "                \"target_vocab\": self.target_vocab.to_serializable(), \n",
    "                \"max_source_length\": self.max_source_length,\n",
    "                \"max_target_length\": self.max_target_length}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    \n",
    "    '''创建一个新闻数据类来对数据进行向量化和分组'''\n",
    "    \n",
    "    def __init__(self, text_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_df: 翻译数据集\n",
    "            vectorizer: 由训练集生成的向量化器\n",
    "        \"\"\"\n",
    "        self.text_df = text_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df = self.text_df.iloc[0:int(len(self.text_df)*0.7)]\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.text_df.iloc[int(len(self.text_df)*0.7):int(len(self.text_df)*0.85)]\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.text_df.iloc[int(len(self.text_df)*0.85):]\n",
    "        self.test_size = len(self.test_df)\n",
    "        # 将数据集分划后保存在dict中，通过set_split调取需要使用的数据集\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "        \n",
    "    # 通过数据集创建数据集实例\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_csv, sampling):\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        text_df = text_df.iloc[0:int(len(text_df)*sampling)]\n",
    "        return cls(text_df, NMTVectorizer.from_dataframe(text_df))\n",
    "\n",
    "    # 通过数据集以及保存好的向量化器来创建数据集实例\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath, sampling):\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        text_df = text_df.iloc[0:int(len(text_df)*sampling)]\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(text_df, vectorizer)\n",
    "    \n",
    "    # 从JSON文件中加载保存好的向量化器\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NMTVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    # 将向量化器保存到JSON文件中\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    # 用于之后的vectorizer提取使用\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    # 根据情况选择当前要使用的数据集，默认使用训练集\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    # 定义数据集的长度，用于DataLoader的batch数量计算\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    # 定义数据集的输出，用于DataLoader的batch数据生成\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        vector_dict = self._vectorizer.vectorize(row['English'], row['Chinese'])\n",
    "        return {\"x_source\": vector_dict[\"source_vector\"], \n",
    "                \"x_target\": vector_dict[\"target_x_vector\"],\n",
    "                \"y_target\": vector_dict[\"target_y_vector\"], \n",
    "                \"x_source_length\": vector_dict[\"source_length\"]}\n",
    "\n",
    "# 升级DataLoader的功能，将其输出批中的句子由长到短排序，方便pack_padded_sequence使用\n",
    "def generate_nmt_batches(dataset, batch_size, shuffle=True, drop_last=True):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['x_source_length'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices]\n",
    "        yield out_data_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTEncoder(nn.Module):\n",
    "    \n",
    "    '''创建一个编码器'''\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_embeddings: 词嵌入矩阵的行数，等于词典单词的数量\n",
    "            embedding_size: 词嵌入矩阵的维度，人为规定大小\n",
    "            rnn_hidden_size: RNN的隐藏层大小\n",
    "        \"\"\"\n",
    "        super(NMTEncoder, self).__init__()\n",
    "        self.source_embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                             embedding_dim=embedding_size,\n",
    "                                             padding_idx=0)\n",
    "        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    # 输入数据批，返回每个时间步长的输出值以及最后的编码\n",
    "    def forward(self, x_source, x_lengths):\n",
    "        x_embedded = self.source_embedding(x_source)\n",
    "        x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(), batch_first=True)\n",
    "        x_birnn_out, x_birnn_h = self.birnn(x_packed)\n",
    "        x_birnn_h = x_birnn_h.permute(1, 0, 2)\n",
    "        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\n",
    "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n",
    "        return x_unpacked, x_birnn_h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建注意力机制\n",
    "def terse_attention(encoder_state_vectors, query_vector):\n",
    "    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueeze(dim=2)).squeeze()\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=-1)\n",
    "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1), \n",
    "                                   vector_probabilities.unsqueeze(dim=2)).squeeze()\n",
    "    return context_vectors, vector_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    \n",
    "    '''创建一个解码器'''\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_embeddings: 词嵌入矩阵的行数，等于词典单词的数量\n",
    "            embedding_size: 词嵌入矩阵的维度，人为规定大小\n",
    "            rnn_hidden_size: RNN的隐藏层大小\n",
    "            bos_index: 句子开头的索引\n",
    "        \"\"\"\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                             embedding_dim=embedding_size,\n",
    "                                             padding_idx=0)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size+rnn_hidden_size, rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\n",
    "        self.bos_index = bos_index\n",
    "    \n",
    "    # 生成由BOS组成的批数据，用于GRU的初始输入\n",
    "    def _init_indices(self, batch_size):\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
    "    \n",
    "    # 生成初始化的注意力机制的上下文向量\n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
    "    \n",
    "    # 输入数据批，返回每个时间步长的输出值\n",
    "    def forward(self, encoder_state, encoder_output, target_sequence, sample_probability=0.0):\n",
    "        if target_sequence is None:\n",
    "            sample_probability = 1.0\n",
    "        else:\n",
    "            target_sequence = target_sequence.permute(1, 0)\n",
    "            output_sequence_size = target_sequence.size(0)\n",
    "        h_t = self.hidden_map(encoder_output)\n",
    "        batch_size = encoder_state.size(0)\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        for i in range(output_sequence_size):\n",
    "            use_sample = np.random.random() < sample_probability\n",
    "            if not use_sample:\n",
    "                y_t_index = target_sequence[i]\n",
    "            y_input_vector = self.target_embedding(y_t_index)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
    "            context_vectors, p_attn = terse_attention(encoder_state_vectors=encoder_state,\n",
    "                                                      query_vector=h_t)\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
    "            if use_sample:\n",
    "                p_y_t_index = F.softmax(score_for_y_t_index, dim=1)\n",
    "                y_t_index = torch.multinomial(p_y_t_index, 1).squeeze()\n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n",
    "        return output_vectors\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    \n",
    "    '''创建机器翻译模型'''\n",
    "    \n",
    "    def __init__(self, source_vocab_size, source_embedding_size,\n",
    "                 target_vocab_size, target_embedding_size, encoding_size,\n",
    "                 target_bos_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_vocab_size: 词嵌入矩阵的行数，等于词典单词的数量\n",
    "            source_embedding_size: 词嵌入矩阵的维度，人为规定大小\n",
    "            target_vocab_size: 词嵌入矩阵的行数，等于词典单词的数量\n",
    "            target_embedding_size: 词嵌入矩阵的维度，人为规定大小\n",
    "            encoding_size: 编码器的隐藏值向量大小\n",
    "            target_bos_index: 翻译结果句子的开头索引\n",
    "        \"\"\"\n",
    "        super(NMTModel, self).__init__()\n",
    "        self.encoder = NMTEncoder(num_embeddings=source_vocab_size,\n",
    "                                  embedding_size=source_embedding_size,\n",
    "                                  rnn_hidden_size=encoding_size)\n",
    "        decoding_size = encoding_size * 2\n",
    "        self.decoder = NMTDecoder(num_embeddings=target_vocab_size,\n",
    "                                  embedding_size=target_embedding_size,\n",
    "                                  rnn_hidden_size=decoding_size,\n",
    "                                  bos_index=target_bos_index)\n",
    "        \n",
    "    # 输入数据批，返回每个时间步长的输出值\n",
    "    def forward(self, x_source, x_source_lengths, target_sequence, sample_probability=0.0):\n",
    "        encoder_state, final_hidden_states = self.encoder(x_source, x_source_lengths)\n",
    "        decoded_states = self.decoder(encoder_state=encoder_state,\n",
    "                                      encoder_output=final_hidden_states,\n",
    "                                      target_sequence=target_sequence,\n",
    "                                      sample_probability=sample_probability)\n",
    "        return decoded_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来跟踪模型的训练过程以及控制训练状态\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每轮训练结束将更新一次训练状态\n",
    "def update_train_state(args, model, train_state):\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_pre_t, loss_t = train_state['val_loss'][-2:]\n",
    "        if loss_t >= loss_pre_t:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        else:\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "            train_state['early_stopping_step'] = 0\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "    return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的准确度\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "    return n_correct / n_valid * 100 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的损失值\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练模型需要的所有工具\n",
    "def prepare_training_process(args):\n",
    "    # 设置概率种子\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    # 初始化保存文件夹\n",
    "    if not os.path.exists('news_folder/'):\n",
    "        os.makedirs('news_folder/')\n",
    "    # 准备数据集\n",
    "    if os.path.exists(args.vectorizer_file):\n",
    "        dataset = NMTDataset.load_dataset_and_load_vectorizer(\n",
    "            args.nmt_csv, args.vectorizer_file, args.sampling)\n",
    "    else:\n",
    "        dataset = NMTDataset.load_dataset_and_make_vectorizer(args.nmt_csv, args.sampling)\n",
    "        dataset.save_vectorizer(args.vectorizer_file)\n",
    "    # 准备向量化器\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    # 准备模型\n",
    "    model = NMTModel(source_vocab_size=len(vectorizer.source_vocab), \n",
    "                     source_embedding_size=args.source_embedding_size, \n",
    "                     target_vocab_size=len(vectorizer.target_vocab),\n",
    "                     target_embedding_size=args.target_embedding_size, \n",
    "                     encoding_size=args.encoding_size,\n",
    "                     target_bos_index=vectorizer.target_vocab.begin_index)\n",
    "    # 准备优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    # 准备损失函数\n",
    "    loss_func = sequence_loss\n",
    "    # 准备学习率调整器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "    return dataset, vectorizer, model, optimizer, loss_func, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型，验证模型，测试模型，保存模型\n",
    "def train_model(args, dataset, model, optimizer, loss_func, scheduler):\n",
    "    mask_index = dataset.get_vectorizer().target_vocab.mask_index\n",
    "    train_state = make_train_state(args)\n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            sample_probability = (20 + epoch_index) / args.num_epochs\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "            # 训连模型并将每轮训练结果用于更新状态\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = generate_nmt_batches(dataset, batch_size=args.batch_size)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            model.train()\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                y_pred = model(batch_dict['x_source'], batch_dict['x_source_length'], \n",
    "                           batch_dict['x_target'], sample_probability=sample_probability)\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'], mask_index)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "            train_state['train_acc'].append(running_acc)\n",
    "            # 验证模型并将验证结果用于更新状态\n",
    "            dataset.set_split('val')\n",
    "            batch_generator = generate_nmt_batches(dataset, batch_size=args.batch_size)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            model.eval()\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                y_pred = model(batch_dict['x_source'], batch_dict['x_source_length'], \n",
    "                           batch_dict['x_target'], sample_probability=sample_probability)\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'], mask_index)\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "            # 更新训练状态\n",
    "            train_state = update_train_state(args=args, model=model, train_state=train_state)\n",
    "            # 更新学习率\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "            # 打印每轮训练的结果\n",
    "            print(\"Epoch: {} / {} -- Train Accuracy: {:.3f}, Val Accuracy: {:.3f}\".format(\n",
    "                train_state['epoch_index']+1, args.num_epochs, train_state['train_acc'][-1], \n",
    "                train_state['val_acc'][-1]))\n",
    "            # 判断是否提前结速训练\n",
    "            if train_state['stop_early']:\n",
    "                print('Early Stop Training!')\n",
    "                break\n",
    "        # 使用测试集测试训练好的模型，更新状态中的测试结果\n",
    "        model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "        dataset.set_split('test')\n",
    "        batch_generator = generate_nmt_batches(dataset, batch_size=args.batch_size)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.eval()\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            y_pred = model(batch_dict['x_source'], batch_dict['x_source_length'], \n",
    "                           batch_dict['x_target'], sample_probability=sample_probability)\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'], mask_index)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "        train_state['test_loss'] = running_loss\n",
    "        train_state['test_acc'] = running_acc\n",
    "        print(\"Test Accuracy: {:.3f}\".format(train_state['test_acc']))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练分类器模型，并保存到文件夹中\n",
    "dataset, vectorizer, model, optimizer, loss_func, scheduler = prepare_training_process(args)\n",
    "train_model(args, dataset, model, optimizer, loss_func, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_sentence(vectorizer, batch_dict, index):\n",
    "    indices = batch_dict['x_source'][index].cpu().data.numpy()\n",
    "    vocab = vectorizer.source_vocab\n",
    "    return sentence_from_indices(indices, vocab)\n",
    "\n",
    "def get_true_sentence(vectorizer, batch_dict, index):\n",
    "    return sentence_from_indices(batch_dict['y_target'].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "    \n",
    "def get_sampled_sentence(vectorizer, batch_dict, index):\n",
    "    y_pred = model(x_source=batch_dict['x_source'], \n",
    "                   x_source_lengths=batch_dict['x_source_length'], \n",
    "                   target_sequence=batch_dict['x_target'], \n",
    "                   sample_probability=1.0)\n",
    "    return sentence_from_indices(torch.max(y_pred, dim=2)[1].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "\n",
    "def get_all_sentences(vectorizer, batch_dict, index):\n",
    "    return {\"source\": get_source_sentence(vectorizer, batch_dict, index), \n",
    "            \"truth\": get_true_sentence(vectorizer, batch_dict, index), \n",
    "            \"sampled\": get_sampled_sentence(vectorizer, batch_dict, index)}\n",
    "    \n",
    "def sentence_from_indices(indices, vocab, strict=True):\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_index and strict:\n",
    "            return \" \".join(out)\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "    return \" \".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_result(args, dataset, sample_size):\n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_nmt_batches(dataset, batch_size=args.batch_size)\n",
    "    batch_dict = next(batch_generator)\n",
    "    result = []\n",
    "    for i in range(sample_size):\n",
    "        results = get_all_sentences(vectorizer, batch_dict, i)\n",
    "        result.append(results)\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_result(args, dataset, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
